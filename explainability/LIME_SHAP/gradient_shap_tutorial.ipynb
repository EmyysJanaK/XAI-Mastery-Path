{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4329ed03",
   "metadata": {},
   "source": [
    "# Explaining WavLM Speech Models with GradientSHAP\n",
    "\n",
    "This notebook demonstrates how to implement and use GradientSHAP for explaining predictions from WavLM speech models using audio data from librosa. We'll walk through the entire process step-by-step:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Understanding how GradientSHAP works\n",
    "3. Loading and preparing WavLM models\n",
    "4. Implementing a dataset class for audio files\n",
    "5. Building the GradientSHAP explainer\n",
    "6. Generating and visualizing attributions\n",
    "\n",
    "By the end of this tutorial, you'll be able to explain which parts of an audio input contribute most to a model's prediction for tasks like speech emotion recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd4cb2",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Import Libraries\n",
    "\n",
    "Let's start by installing the necessary libraries for our implementation. We'll need:\n",
    "\n",
    "- PyTorch: For deep learning and gradient computation\n",
    "- Transformers: To access the WavLM model and feature extractors\n",
    "- Librosa: For audio processing and manipulation\n",
    "- Matplotlib/Seaborn: For visualization\n",
    "- NumPy: For numerical operations\n",
    "- tqdm: For progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a47d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchaudio transformers librosa matplotlib seaborn numpy tqdm\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from typing import Dict, List, Tuple, Union, Optional, Any, Callable\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set up seaborn style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9524cf",
   "metadata": {},
   "source": [
    "## 2. Understanding GradientSHAP\n",
    "\n",
    "GradientSHAP is an efficient approximation method that combines ideas from Integrated Gradients and SHAP (SHapley Additive exPlanations). It helps us understand which parts of an input most influence a model's predictions.\n",
    "\n",
    "### How GradientSHAP Works:\n",
    "\n",
    "1. **Reference Distribution**: Create a reference distribution by generating samples that interpolate between random noise and the actual input.\n",
    "\n",
    "2. **Gradient Computation**: Calculate gradients of the model output with respect to the inputs for each sample.\n",
    "\n",
    "3. **Weighted Integration**: Combine these gradients using weights according to SHAP principles to get attribution scores.\n",
    "\n",
    "For speech models like WavLM, GradientSHAP can tell us:\n",
    "- Which frames (time segments) of audio most influence predictions\n",
    "- Which neurons in the model are most responsive to important audio features\n",
    "\n",
    "### Key Advantages:\n",
    "\n",
    "- More computationally efficient than kernel SHAP methods\n",
    "- Provides local explanations for specific predictions\n",
    "- Works well with deep neural networks that have gradient information\n",
    "- Can handle high-dimensional inputs like speech spectrograms\n",
    "\n",
    "Let's see how to implement this for WavLM speech models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f68f4a",
   "metadata": {},
   "source": [
    "## 3. Load WavLM Model and Feature Extractor\n",
    "\n",
    "Now let's load a pre-trained WavLM model and its feature extractor from HuggingFace. WavLM (Wave-Language Model) is a powerful speech model that can be fine-tuned for various downstream tasks like speech recognition, speaker verification, or emotion detection.\n",
    "\n",
    "For our example, we'll use WavLM-Base-Plus, which has 12 transformer layers and is a good balance between performance and resource usage. We'll also create a simple classifier head that can be used for emotion recognition (with 8 emotion classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79203921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WavLM model and feature extractor\n",
    "model_name = 'microsoft/wavlm-base-plus'\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "# Load model\n",
    "wavlm_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "wavlm_model.to(device)\n",
    "\n",
    "# Put model in evaluation mode\n",
    "wavlm_model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model architecture: {type(wavlm_model).__name__}\")\n",
    "print(f\"Hidden size: {wavlm_model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {len(wavlm_model.encoder.layers)}\")\n",
    "\n",
    "# Create a simple classifier head for emotion recognition (8 classes)\n",
    "# In a real scenario, you would train this classifier on labeled data\n",
    "hidden_size = wavlm_model.config.hidden_size\n",
    "num_classes = 8  # 8 emotion classes (e.g., neutral, happy, sad, angry, etc.)\n",
    "\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(hidden_size, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(256, num_classes)\n",
    ").to(device)\n",
    "\n",
    "print(f\"Classifier architecture:\")\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435cc5d",
   "metadata": {},
   "source": [
    "## 4. Create LibrosaAudioDataset Class\n",
    "\n",
    "Next, we'll implement a PyTorch dataset class for loading audio files using librosa. This class will:\n",
    "\n",
    "1. Load audio files from a directory\n",
    "2. Resample them to the required sample rate for WavLM (16kHz)\n",
    "3. Process them with the WavLM feature extractor\n",
    "4. Return tensors ready for model input\n",
    "\n",
    "This dataset class will help us process multiple audio files efficiently, whether for training a classifier or for batch explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012febd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibrosaAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading audio files using librosa.\n",
    "    \n",
    "    This dataset can load audio files from a directory and prepare them for\n",
    "    use with WavLM and similar speech models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, feature_extractor, labels=None, sample_rate=16000, max_samples=None, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to audio files directory\n",
    "            feature_extractor: WavLM feature extractor for audio preprocessing\n",
    "            labels (dict, optional): Dictionary mapping filenames to labels\n",
    "            sample_rate (int): Target sample rate for audio\n",
    "            max_samples (int, optional): Maximum number of samples to use\n",
    "            transform (callable, optional): Optional transform to apply to audio\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "        self.labels = labels or {}\n",
    "        \n",
    "        # List all audio files\n",
    "        self.audio_files = []\n",
    "        \n",
    "        # Walk through the directory\n",
    "        for root, _, files in os.walk(data_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.wav', '.mp3', '.flac')):\n",
    "                    self.audio_files.append(os.path.join(root, file))\n",
    "        \n",
    "        # Limit dataset size if specified\n",
    "        if max_samples and max_samples < len(self.audio_files):\n",
    "            self.audio_files = random.sample(self.audio_files, max_samples)\n",
    "            \n",
    "        print(f\"Loaded {len(self.audio_files)} audio files from {data_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary with input_values, label (if available), and filename\n",
    "        \"\"\"\n",
    "        audio_path = self.audio_files[idx]\n",
    "        filename = os.path.basename(audio_path)\n",
    "        \n",
    "        # Load and resample audio\n",
    "        waveform, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        \n",
    "        # Apply transform if specified\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        # Convert to float32 tensor\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "        \n",
    "        # Process with feature extractor\n",
    "        inputs = self.feature_extractor(waveform, sampling_rate=self.sample_rate, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get label if available\n",
    "        label = self.labels.get(filename, -1)\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(0),\n",
    "            \"label\": label,\n",
    "            \"filename\": filename\n",
    "        }\n",
    "\n",
    "# Test the dataset class with a small example\n",
    "# Uncomment this if you have audio files available\n",
    "\"\"\"\n",
    "# Create a small test dataset (replace with your audio directory)\n",
    "test_audio_path = \"./sample_audio\"\n",
    "if os.path.exists(test_audio_path):\n",
    "    # Create dataset\n",
    "    test_dataset = LibrosaAudioDataset(\n",
    "        data_path=test_audio_path,\n",
    "        feature_extractor=feature_extractor,\n",
    "        max_samples=2\n",
    "    )\n",
    "    \n",
    "    # Get a sample\n",
    "    if len(test_dataset) > 0:\n",
    "        sample = test_dataset[0]\n",
    "        print(f\"Sample filename: {sample['filename']}\")\n",
    "        print(f\"Input shape: {sample['input_values'].shape}\")\n",
    "else:\n",
    "    print(\"No test audio directory available. Skipping dataset test.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e202b6",
   "metadata": {},
   "source": [
    "## 5. Implement GradientSHAP Explainer\n",
    "\n",
    "Now we'll implement the core of our explainer: the GradientSHAP class. This class will:\n",
    "\n",
    "1. Register hooks to capture activations from different layers\n",
    "2. Generate reference samples by interpolating between random noise and the input\n",
    "3. Compute gradients for each sample\n",
    "4. Calculate SHAP values and aggregate them\n",
    "5. Provide visualization utilities\n",
    "\n",
    "This is a comprehensive implementation that supports both frame-level and neuron-level explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5897671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientSHAP:\n",
    "    \"\"\"\n",
    "    GradientSHAP implementation for WavLM speech models.\n",
    "    \n",
    "    This class implements GradientSHAP for speech models by:\n",
    "    1. Creating reference samples by mixing the target input with random noise\n",
    "    2. Computing gradients for each reference sample\n",
    "    3. Weighting the gradients according to SHAP principles\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        feature_extractor,\n",
    "        device=None,\n",
    "        num_samples=50,\n",
    "        feature_layer=None,\n",
    "        classifier=None,\n",
    "        batch_size=8\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the GradientSHAP explainer.\n",
    "        \n",
    "        Args:\n",
    "            model: Pre-loaded WavLM model\n",
    "            feature_extractor: WavLM feature extractor\n",
    "            device: Device to use ('cuda' or 'cpu')\n",
    "            num_samples: Number of reference samples for SHAP\n",
    "            feature_layer: Layer to extract features from (None for last layer)\n",
    "            classifier: Classification head\n",
    "            batch_size: Batch size for processing reference samples\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"GradientSHAP using device: {self.device}\")\n",
    "        \n",
    "        # Store model and feature extractor\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        # Ensure model is in evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Parameters\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_layer = feature_layer\n",
    "        \n",
    "        # Set up classifier if not provided\n",
    "        if classifier is None:\n",
    "            hidden_size = self.model.config.hidden_size\n",
    "            self.classifier = torch.nn.Linear(hidden_size, 8)  # Default: 8 emotions\n",
    "        else:\n",
    "            self.classifier = classifier\n",
    "            \n",
    "        self.classifier.to(self.device)\n",
    "        \n",
    "        # For storing activations\n",
    "        self.activation_store = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Register hooks for specified feature layer\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks to capture activations from the model.\"\"\"\n",
    "        try:\n",
    "            def get_activation(name):\n",
    "                def hook(module, input, output):\n",
    "                    # Store activations - supports both tuple and tensor outputs\n",
    "                    if isinstance(output, tuple):\n",
    "                        self.activation_store[name] = output[0].detach()\n",
    "                    else:\n",
    "                        self.activation_store[name] = output.detach()\n",
    "                return hook\n",
    "            \n",
    "            # Register hooks for specific or all encoder layers\n",
    "            if self.feature_layer is not None:\n",
    "                # Register hook for just the specified layer\n",
    "                if 0 <= self.feature_layer < len(self.model.encoder.layers):\n",
    "                    layer = self.model.encoder.layers[self.feature_layer]\n",
    "                    h = layer.register_forward_hook(get_activation(self.feature_layer))\n",
    "                    self.hooks.append(h)\n",
    "                    print(f\"Registered hook for layer {self.feature_layer}\")\n",
    "            else:\n",
    "                # Register hooks for all encoder layers\n",
    "                for i, layer in enumerate(self.model.encoder.layers):\n",
    "                    h = layer.register_forward_hook(get_activation(i))\n",
    "                    self.hooks.append(h)\n",
    "                print(f\"Registered hooks for all {len(self.hooks)} encoder layers\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error registering hooks: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "        print(\"Removed all hooks\")\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up by removing hooks when object is deleted\"\"\"\n",
    "        if hasattr(self, 'hooks') and self.hooks:\n",
    "            self.remove_hooks()\n",
    "    \n",
    "    def generate_reference_samples(self, input_values, num_samples=None):\n",
    "        \"\"\"\n",
    "        Generate reference samples by interpolating between input and random noise.\n",
    "        \n",
    "        Args:\n",
    "            input_values: Input audio values [T]\n",
    "            num_samples: Number of samples to generate\n",
    "            \n",
    "        Returns:\n",
    "            Reference samples [N, T]\n",
    "            Interpolation coefficients [N, 1]\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "            \n",
    "        # Create random reference noise with same shape as input\n",
    "        # Use a normal distribution with same mean and std as input\n",
    "        input_mean = input_values.mean()\n",
    "        input_std = input_values.std()\n",
    "        \n",
    "        # Create reference distribution (random noise)\n",
    "        reference = torch.normal(\n",
    "            mean=input_mean,\n",
    "            std=input_std,\n",
    "            size=(num_samples, input_values.shape[0])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Create alphas for interpolation (0 = reference, 1 = input)\n",
    "        alphas = torch.linspace(0, 1, num_samples).view(-1, 1).to(self.device)\n",
    "        \n",
    "        # Generate samples by interpolating between reference and input\n",
    "        samples = reference * (1 - alphas) + input_values * alphas\n",
    "        \n",
    "        return samples, alphas\n",
    "    \n",
    "    def _compute_sample_gradients(self, samples, target_class=None):\n",
    "        \"\"\"\n",
    "        Compute gradients for sample inputs with respect to the target class.\n",
    "        \n",
    "        Args:\n",
    "            samples: Batch of input samples [B, T]\n",
    "            target_class: Target class for explanation\n",
    "            \n",
    "        Returns:\n",
    "            Gradients for each sample\n",
    "        \"\"\"\n",
    "        batch_size = samples.shape[0]\n",
    "        gradients = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            sample = samples[i:i+1]  # Keep batch dimension [1, T]\n",
    "            sample.requires_grad_(True)\n",
    "            \n",
    "            # Process with feature extractor\n",
    "            inputs = self.feature_extractor(sample[0], sampling_rate=16000, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            self.model.zero_grad()\n",
    "            if hasattr(self.classifier, 'zero_grad'):\n",
    "                self.classifier.zero_grad()\n",
    "            \n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get predictions from classifier\n",
    "            if hasattr(outputs, 'last_hidden_state'):\n",
    "                pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "                logits = self.classifier(pooled)\n",
    "                \n",
    "                # Get target class if not specified\n",
    "                if target_class is None:\n",
    "                    target_class = logits.argmax(dim=1).item()\n",
    "                \n",
    "                # Get target class logit\n",
    "                target_logit = logits[0, target_class]\n",
    "                \n",
    "                # Backward pass to get gradients\n",
    "                target_logit.backward()\n",
    "                \n",
    "                # Get gradients with respect to input\n",
    "                grad = sample.grad.detach()\n",
    "                gradients.append(grad)\n",
    "            \n",
    "            # Clean up\n",
    "            if sample.grad is not None:\n",
    "                sample.grad.zero_()\n",
    "        \n",
    "        # Combine gradients\n",
    "        return torch.cat(gradients, dim=0)\n",
    "    \n",
    "    def _compute_shap_values(self, gradients, alphas, input_values, mode='frame', aggregate_frames='mean'):\n",
    "        \"\"\"\n",
    "        Compute final SHAP values from gradients.\n",
    "        \n",
    "        Args:\n",
    "            gradients: Gradients from all samples [N, T]\n",
    "            alphas: Interpolation coefficients [N, 1]\n",
    "            input_values: Original input values [T]\n",
    "            mode: 'frame' or 'neuron' level explanations\n",
    "            aggregate_frames: Method to aggregate frame attributions\n",
    "            \n",
    "        Returns:\n",
    "            SHAP values for the input\n",
    "        \"\"\"\n",
    "        # Apply trapezoidal rule for integration\n",
    "        shap_values = gradients * input_values\n",
    "        \n",
    "        # For neuron-level explanations, we need to access the activation store\n",
    "        if mode == 'neuron' and self.activation_store:\n",
    "            # Get activation from the latest forward pass\n",
    "            # We need to do one more forward pass to capture the activations\n",
    "            with torch.no_grad():\n",
    "                inputs = self.feature_extractor(input_values, sampling_rate=16000, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                _ = self.model(**inputs)\n",
    "            \n",
    "            # Extract neuron-level explanations\n",
    "            neuron_attributions = {}\n",
    "            \n",
    "            for layer_idx, activations in self.activation_store.items():\n",
    "                # If we are only interested in specific layer\n",
    "                if self.feature_layer is not None and layer_idx != self.feature_layer:\n",
    "                    continue\n",
    "                    \n",
    "                # Get shape information\n",
    "                batch_size, seq_len, hidden_dim = activations.shape\n",
    "                \n",
    "                # For each neuron, compute importance\n",
    "                layer_attributions = torch.zeros((seq_len, hidden_dim)).to(self.device)\n",
    "                \n",
    "                # We use the integrated gradients to estimate neuron importance\n",
    "                # This is a simplification - in practice we'd need to compute\n",
    "                # gradients w.r.t each neuron activation\n",
    "                for frame_idx in range(seq_len):\n",
    "                    for neuron_idx in range(hidden_dim):\n",
    "                        # Estimate neuron importance by corresponding gradient\n",
    "                        neuron_attr = gradients[:, frame_idx].sum()\n",
    "                        layer_attributions[frame_idx, neuron_idx] = neuron_attr\n",
    "                \n",
    "                neuron_attributions[f\"layer_{layer_idx}\"] = layer_attributions.cpu().numpy()\n",
    "            \n",
    "            return neuron_attributions\n",
    "        \n",
    "        # For frame-level explanations, we aggregate across frames\n",
    "        else:\n",
    "            # Convert to numpy for easier handling\n",
    "            frame_attributions = shap_values.cpu().numpy()\n",
    "            \n",
    "            # Aggregate if requested\n",
    "            if aggregate_frames == 'mean':\n",
    "                return frame_attributions.mean(axis=0)\n",
    "            elif aggregate_frames == 'sum':\n",
    "                return frame_attributions.sum(axis=0)\n",
    "            elif aggregate_frames == 'max':\n",
    "                return frame_attributions.max(axis=0)\n",
    "            else:\n",
    "                return frame_attributions\n",
    "    \n",
    "    def explain(\n",
    "        self, \n",
    "        input_values, \n",
    "        target_class=None,\n",
    "        mode='frame',\n",
    "        aggregate_frames='mean'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate GradientSHAP explanations for a given input.\n",
    "        \n",
    "        Args:\n",
    "            input_values: Input audio values\n",
    "            target_class: Target class for explanation\n",
    "            mode: 'frame' or 'neuron' level explanations\n",
    "            aggregate_frames: Method to aggregate frame attributions\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Explanation results including attributions\n",
    "        \"\"\"\n",
    "        print(f\"Generating GradientSHAP explanations in {mode} mode\")\n",
    "        \n",
    "        # Move input to device\n",
    "        if not isinstance(input_values, torch.Tensor):\n",
    "            input_values = torch.tensor(input_values, dtype=torch.float32)\n",
    "        \n",
    "        input_values = input_values.to(self.device)\n",
    "        \n",
    "        # Generate reference samples\n",
    "        samples, alphas = self.generate_reference_samples(input_values)\n",
    "        num_samples = samples.shape[0]\n",
    "        \n",
    "        # Process in batches\n",
    "        all_gradients = []\n",
    "        \n",
    "        for batch_idx in tqdm(range(0, num_samples, self.batch_size), desc=\"Computing gradients\"):\n",
    "            batch_end = min(batch_idx + self.batch_size, num_samples)\n",
    "            batch_samples = samples[batch_idx:batch_end]\n",
    "            batch_alphas = alphas[batch_idx:batch_end]\n",
    "            \n",
    "            # Get gradients for this batch\n",
    "            batch_gradients = self._compute_sample_gradients(batch_samples, target_class)\n",
    "            all_gradients.append(batch_gradients)\n",
    "        \n",
    "        # Combine gradients from all batches\n",
    "        all_gradients = torch.cat(all_gradients, dim=0)\n",
    "        \n",
    "        # Compute integrated gradients by weighting according to SHAP formulation\n",
    "        shap_values = self._compute_shap_values(all_gradients, alphas, input_values, mode, aggregate_frames)\n",
    "        \n",
    "        # Run forward pass to get predictions\n",
    "        with torch.no_grad():\n",
    "            inputs = self.feature_extractor(input_values, sampling_rate=16000, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get class predictions using classifier\n",
    "            if hasattr(outputs, 'last_hidden_state'):\n",
    "                pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "                logits = self.classifier(pooled)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                # Get predicted class if target not specified\n",
    "                if target_class is None:\n",
    "                    target_class = torch.argmax(probs, dim=1).item()\n",
    "                \n",
    "                prediction = {\n",
    "                    'class': target_class,\n",
    "                    'probability': probs[0, target_class].item()\n",
    "                }\n",
    "            else:\n",
    "                prediction = {'class': -1, 'probability': 0.0}\n",
    "            \n",
    "        return {\n",
    "            'attributions': shap_values,\n",
    "            'prediction': prediction,\n",
    "            'mode': mode,\n",
    "            'aggregate_method': aggregate_frames,\n",
    "        }\n",
    "\n",
    "# Create an instance of GradientSHAP\n",
    "explainer = GradientSHAP(\n",
    "    model=wavlm_model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    device=device,\n",
    "    num_samples=20,  # Using a small number for demonstration\n",
    "    classifier=classifier,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(\"GradientSHAP explainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0ff9d",
   "metadata": {},
   "source": [
    "## 6. Working with Audio Files\n",
    "\n",
    "Now that we have our GradientSHAP explainer set up, let's load a sample audio file and prepare it for explanation. We'll use librosa to load and preprocess the audio, then feed it into our explainer.\n",
    "\n",
    "For this demonstration, we'll create a synthetic audio sample if you don't have one available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43346b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a synthetic audio sample for demonstration\n",
    "def create_synthetic_audio(duration=3, sr=16000):\n",
    "    \"\"\"Create a synthetic audio sample with some speech-like characteristics.\"\"\"\n",
    "    # Create time array\n",
    "    t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
    "    \n",
    "    # Generate a mixture of frequencies that roughly mimics speech formants\n",
    "    freqs = [120, 240, 500, 1000, 2000]\n",
    "    amplitudes = [1.0, 0.5, 0.3, 0.2, 0.1]\n",
    "    \n",
    "    # Create base signal\n",
    "    signal = np.zeros_like(t)\n",
    "    for freq, amp in zip(freqs, amplitudes):\n",
    "        signal += amp * np.sin(2 * np.pi * freq * t)\n",
    "    \n",
    "    # Add some amplitude modulation to simulate speech syllables\n",
    "    syllable_rate = 4  # 4 syllables per second\n",
    "    modulation = 0.5 + 0.5 * np.sin(2 * np.pi * syllable_rate * t)\n",
    "    signal *= modulation\n",
    "    \n",
    "    # Add some noise\n",
    "    noise = np.random.normal(0, 0.05, size=len(t))\n",
    "    signal += noise\n",
    "    \n",
    "    # Normalize\n",
    "    signal = signal / np.max(np.abs(signal))\n",
    "    \n",
    "    return signal, sr\n",
    "\n",
    "# Create a synthetic audio sample\n",
    "audio_signal, sample_rate = create_synthetic_audio(duration=3, sr=16000)\n",
    "\n",
    "# Visualize the audio\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.arange(len(audio_signal)) / sample_rate, audio_signal)\n",
    "plt.title(\"Synthetic Audio Sample\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Display spectrogram\n",
    "plt.figure(figsize=(12, 4))\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_signal)), ref=np.max)\n",
    "librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Spectrogram of Synthetic Audio\")\n",
    "plt.show()\n",
    "\n",
    "# Convert to tensor for the model\n",
    "audio_tensor = torch.tensor(audio_signal, dtype=torch.float32)\n",
    "\n",
    "print(f\"Audio shape: {audio_signal.shape}\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_signal) / sample_rate:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a63ab",
   "metadata": {},
   "source": [
    "## 7. Generating Explanations\n",
    "\n",
    "Now let's use our GradientSHAP implementation to generate explanations for the audio sample. We'll demonstrate:\n",
    "\n",
    "1. Frame-level explanations that show which parts of the audio most influence the model's prediction\n",
    "2. How to interpret the attribution scores\n",
    "\n",
    "We'll target emotion class 2 (happy) for this example, but in a real application, you might want to explain the predicted class or compare explanations across multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate frame-level explanations using GradientSHAP\n",
    "target_class = 2  # Target \"happy\" emotion (adjust as needed)\n",
    "\n",
    "# Generate explanations\n",
    "explanation_results = explainer.explain(\n",
    "    input_values=audio_tensor,\n",
    "    target_class=target_class,\n",
    "    mode='frame',\n",
    "    aggregate_frames=None  # Don't aggregate, keep all attributions\n",
    ")\n",
    "\n",
    "# Extract attributions and prediction info\n",
    "attributions = explanation_results['attributions']\n",
    "prediction = explanation_results['prediction']\n",
    "\n",
    "print(f\"Prediction: Class {prediction['class']} with probability {prediction['probability']:.4f}\")\n",
    "print(f\"Attribution shape: {attributions.shape}\")\n",
    "\n",
    "# Calculate summary statistics for attributions\n",
    "if attributions.ndim > 1:\n",
    "    # If we have multiple attribution values (e.g., for each sample)\n",
    "    # Compute absolute values and mean across samples\n",
    "    abs_attr = np.abs(attributions)\n",
    "    mean_attr = abs_attr.mean(axis=0)\n",
    "    std_attr = abs_attr.std(axis=0)\n",
    "    \n",
    "    print(f\"Mean absolute attribution: {mean_attr.mean():.6f}\")\n",
    "    print(f\"Max absolute attribution: {mean_attr.max():.6f}\")\n",
    "else:\n",
    "    # If we have a single attribution vector\n",
    "    abs_attr = np.abs(attributions)\n",
    "    \n",
    "    print(f\"Mean absolute attribution: {abs_attr.mean():.6f}\")\n",
    "    print(f\"Max absolute attribution: {abs_attr.max():.6f}\")\n",
    "    \n",
    "    # Find top 5 frames with highest attribution\n",
    "    top_indices = np.argsort(abs_attr)[-5:]\n",
    "    top_times = top_indices / sample_rate\n",
    "    \n",
    "    print(\"\\nTop 5 most influential frames:\")\n",
    "    for i, (idx, time) in enumerate(zip(top_indices, top_times), 1):\n",
    "        print(f\"{i}. Frame {idx} at time {time:.3f}s - Attribution: {abs_attr[idx]:.6f}\")\n",
    "\n",
    "# Generate neuron-level explanations for a specific layer\n",
    "# This can be very resource-intensive, so we'll just show the code\n",
    "\"\"\"\n",
    "# Generate neuron-level explanations\n",
    "neuron_explanations = explainer.explain(\n",
    "    input_values=audio_tensor,\n",
    "    target_class=target_class,\n",
    "    mode='neuron',\n",
    ")\n",
    "\n",
    "# Extract attributions for a specific layer\n",
    "layer_idx = 6  # Middle layer\n",
    "if f\"layer_{layer_idx}\" in neuron_explanations['attributions']:\n",
    "    layer_attrs = neuron_explanations['attributions'][f\"layer_{layer_idx}\"]\n",
    "    print(f\"Layer {layer_idx} neuron attributions shape: {layer_attrs.shape}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156dc83",
   "metadata": {},
   "source": [
    "## 8. Visualizing Attributions\n",
    "\n",
    "Finally, let's create a comprehensive visualization that shows the attributions alongside the audio waveform and spectrogram. This will help us understand which parts of the audio are most important for the model's prediction.\n",
    "\n",
    "We'll create a function to visualize frame-level attributions and highlight the most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aea4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio_attributions(audio, attributions, sample_rate, target_class, prediction_prob, top_k=20):\n",
    "    \"\"\"\n",
    "    Visualize attributions for audio with waveform and spectrogram.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio waveform\n",
    "        attributions: Attribution scores from GradientSHAP\n",
    "        sample_rate: Audio sample rate\n",
    "        target_class: Target class for explanation\n",
    "        prediction_prob: Prediction probability for the target class\n",
    "        top_k: Number of top frames to highlight\n",
    "    \"\"\"\n",
    "    # Create time array\n",
    "    times = np.arange(len(audio)) / sample_rate\n",
    "    \n",
    "    # Process attributions\n",
    "    if attributions.ndim > 1:\n",
    "        # Take absolute value and mean across batch dimension\n",
    "        attr_agg = np.abs(attributions).mean(axis=0)\n",
    "    else:\n",
    "        attr_agg = np.abs(attributions)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    attr_norm = attr_agg / (attr_agg.max() + 1e-10)\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
    "    \n",
    "    # Plot 1: Audio waveform\n",
    "    axes[0].plot(times, audio, color='blue', alpha=0.7)\n",
    "    axes[0].set_title(f\"Audio Waveform - Target: Class {target_class} (Probability: {prediction_prob:.4f})\")\n",
    "    axes[0].set_ylabel(\"Amplitude\")\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='hz', ax=axes[1])\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    axes[1].set_title(\"Spectrogram\")\n",
    "    \n",
    "    # Plot 3: Attributions\n",
    "    axes[2].plot(times, attr_norm, color='red', alpha=0.8)\n",
    "    axes[2].set_title(\"Frame-wise GradientSHAP Attribution\")\n",
    "    axes[2].set_ylabel(\"Normalized Attribution\")\n",
    "    axes[2].set_xlabel(\"Time (s)\")\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    # Highlight top k frames\n",
    "    if top_k > 0 and top_k < len(attr_norm):\n",
    "        top_indices = np.argsort(attr_norm)[-top_k:]\n",
    "        axes[2].scatter(times[top_indices], attr_norm[top_indices], \n",
    "                     color='darkred', s=50, zorder=10, label=f\"Top {top_k} frames\")\n",
    "        \n",
    "        # Mark these points on the waveform and spectrogram too\n",
    "        for idx in top_indices:\n",
    "            t = times[idx]\n",
    "            # Vertical lines across all subplots\n",
    "            for ax in axes:\n",
    "                ax.axvline(x=t, color='darkred', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add legend to the last plot\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Visualize the attributions\n",
    "fig = visualize_audio_attributions(\n",
    "    audio=audio_signal, \n",
    "    attributions=attributions,\n",
    "    sample_rate=sample_rate,\n",
    "    target_class=target_class,\n",
    "    prediction_prob=prediction['probability'],\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# Optional: Save the figure\n",
    "# fig.savefig('gradientshap_explanation.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd73215",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've implemented and demonstrated GradientSHAP for explaining WavLM speech model predictions. We've covered:\n",
    "\n",
    "1. The theory behind GradientSHAP and how it combines ideas from Integrated Gradients and SHAP\n",
    "2. How to load and prepare WavLM models and audio data\n",
    "3. Implementation of a complete GradientSHAP explainer for speech models\n",
    "4. How to generate and visualize frame-level and neuron-level attributions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further explore this topic, you might want to:\n",
    "\n",
    "1. Apply this to real audio datasets like RAVDESS for emotion recognition\n",
    "2. Compare attributions across different emotion classes to understand what makes certain emotions distinct\n",
    "3. Extend the implementation to support other speech models like Wav2Vec2 or HuBERT\n",
    "4. Use neuron-level attributions to understand which features the model learns at different layers\n",
    "5. Compare GradientSHAP with other explainability methods like LIME or Integrated Gradients\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [SHAP Library](https://github.com/slundberg/shap) - Includes implementations of many SHAP variants\n",
    "- [WavLM Paper](https://arxiv.org/abs/2110.13900) - Details on the WavLM model architecture\n",
    "- [Captum](https://captum.ai/) - PyTorch model interpretability library with many explainability algorithms"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
